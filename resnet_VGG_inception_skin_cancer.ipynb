{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["%matplotlib inline\n","# python libraties\n","import os, cv2,itertools\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from glob import glob\n","from PIL import Image\n","\n","# pytorch libraries\n","import torch\n","from torch import optim,nn\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader,Dataset\n","from torchvision import models,transforms\n","\n","# sklearn libraries\n","from sklearn.metrics import confusion_matrix\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","\n","# to make the results are reproducible\n","np.random.seed(10)\n","torch.manual_seed(10)\n","torch.cuda.manual_seed(10)\n","\n","print(os.listdir(\"../input\"))"]},{"cell_type":"markdown","metadata":{},"source":["## Step 1. Data analysis and preprocessing"]},{"cell_type":"markdown","metadata":{},"source":["Get the all image data pathsï¼Œ match the row information in HAM10000_metadata.csv with its corresponding image"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[],"source":["data_dir = '../input'\n","all_image_path = glob(os.path.join(data_dir, '*', '*.jpg'))\n","imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x for x in all_image_path}\n","lesion_type_dict = {\n","    'nv': 'Melanocytic nevi',\n","    'mel': 'dermatofibroma',\n","    'bkl': 'Benign keratosis-like lesions ',\n","    'bcc': 'Basal cell carcinoma',\n","    'akiec': 'Actinic keratoses',\n","    'vasc': 'Vascular lesions',\n","    'df': 'Dermatofibroma'\n","}"]},{"cell_type":"markdown","metadata":{},"source":["This function is used to compute the mean and standard deviation on the whole dataset, will use for inputs normalization"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def compute_img_mean_std(image_paths):\n","    \"\"\"\n","        computing the mean and std of three channel on the whole dataset,\n","        first we should normalize the image from 0-255 to 0-1\n","    \"\"\"\n","\n","    img_h, img_w = 224, 224\n","    imgs = []\n","    means, stdevs = [], []\n","\n","    for i in tqdm(range(len(image_paths))):\n","        img = cv2.imread(image_paths[i])\n","        img = cv2.resize(img, (img_h, img_w))\n","        imgs.append(img)\n","\n","    imgs = np.stack(imgs, axis=3)\n","    print(imgs.shape)\n","\n","    imgs = imgs.astype(np.float32) / 255.\n","\n","    for i in range(3):\n","        pixels = imgs[:, :, i, :].ravel()  # resize to one row\n","        means.append(np.mean(pixels))\n","        stdevs.append(np.std(pixels))\n","\n","    means.reverse()  # BGR --> RGB\n","    stdevs.reverse()\n","\n","    print(\"normMean = {}\".format(means))\n","    print(\"normStd = {}\".format(stdevs))\n","    return means,stdevs"]},{"cell_type":"markdown","metadata":{},"source":["Return the mean and std of RGB channels"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["norm_mean,norm_std = compute_img_mean_std(all_image_path)"]},{"cell_type":"markdown","metadata":{},"source":["Add three columns to the original DataFrame, path (image path), cell_type (the whole name),cell_type_idx (the corresponding index  of cell type, as the image label )"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_original = pd.read_csv(os.path.join(data_dir, 'HAM10000_metadata.csv'))\n","df_original['path'] = df_original['image_id'].map(imageid_path_dict.get)\n","df_original['cell_type'] = df_original['dx'].map(lesion_type_dict.get)\n","df_original['cell_type_idx'] = pd.Categorical(df_original['cell_type']).codes\n","df_original.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# this will tell us how many images are associated with each lesion_id\n","df_undup = df_original.groupby('lesion_id').count()\n","# now we filter out lesion_id's that have only one image associated with it\n","df_undup = df_undup[df_undup['image_id'] == 1]\n","df_undup.reset_index(inplace=True)\n","df_undup.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# here we identify lesion_id's that have duplicate images and those that have only one image.\n","def get_duplicates(x):\n","    unique_list = list(df_undup['lesion_id'])\n","    if x in unique_list:\n","        return 'unduplicated'\n","    else:\n","        return 'duplicated'\n","\n","# create a new colum that is a copy of the lesion_id column\n","df_original['duplicates'] = df_original['lesion_id']\n","# apply the function to this new column\n","df_original['duplicates'] = df_original['duplicates'].apply(get_duplicates)\n","df_original.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_original['duplicates'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# now we filter out images that don't have duplicates\n","df_undup = df_original[df_original['duplicates'] == 'unduplicated']\n","df_undup.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# now we create a val set using df because we are sure that none of these images have augmented duplicates in the train set\n","y = df_undup['cell_type_idx']\n","_, df_val = train_test_split(df_undup, test_size=0.2, random_state=101, stratify=y)\n","df_val.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_val['cell_type_idx'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# This set will be df_original excluding all rows that are in the val set\n","# This function identifies if an image is part of the train or val set.\n","def get_val_rows(x):\n","    # create a list of all the lesion_id's in the val set\n","    val_list = list(df_val['image_id'])\n","    if str(x) in val_list:\n","        return 'val'\n","    else:\n","        return 'train'\n","\n","# identify train and val rows\n","# create a new colum that is a copy of the image_id column\n","df_original['train_or_val'] = df_original['image_id']\n","# apply the function to this new column\n","df_original['train_or_val'] = df_original['train_or_val'].apply(get_val_rows)\n","# filter out train rows\n","df_train = df_original[df_original['train_or_val'] == 'train']\n","print(len(df_train))\n","print(len(df_val))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_train['cell_type_idx'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_val['cell_type'].value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["**From From the above statistics of each category, we can see that there is a serious class imbalance in the training data. To solve this problem, I think we can start from two aspects, one is equalization sampling, and the other is a loss function that can be used to mitigate category imbalance during training, such as focal loss.**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Copy fewer class to balance the number of 7 classes\n","data_aug_rate = [15,10,5,50,0,40,5]\n","for i in range(7):\n","    if data_aug_rate[i]:\n","        df_train=df_train.append([df_train.loc[df_train['cell_type_idx'] == i,:]]*(data_aug_rate[i]-1), ignore_index=True)\n","df_train['cell_type'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# # We can split the test set again in a validation set and a true test set:\n","# df_val, df_test = train_test_split(df_val, test_size=0.5)\n","df_train = df_train.reset_index()\n","df_val = df_val.reset_index()\n","# df_test = df_test.reset_index()"]},{"cell_type":"markdown","metadata":{},"source":["## Step 2. Model building"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# feature_extract is a boolean that defines if we are finetuning or feature extracting. \n","# If feature_extract = False, the model is finetuned and all model parameters are updated. \n","# If feature_extract = True, only the last layer parameters are updated, the others remain fixed.\n","def set_parameter_requires_grad(model, feature_extracting):\n","    if feature_extracting:\n","        for param in model.parameters():\n","            param.requires_grad = False"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n","    # Initialize these variables which will be set in this if statement. Each of these\n","    #   variables is model specific.\n","    model_ft = None\n","    input_size = 0\n","\n","    if model_name == \"resnet\":\n","        \"\"\" Resnet18, resnet34, resnet50, resnet101\n","        \"\"\"\n","        model_ft = models.resnet50(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.fc.in_features\n","        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n","        input_size = 224\n","\n","\n","    elif model_name == \"vgg\":\n","        \"\"\" VGG11_bn\n","        \"\"\"\n","        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.classifier[6].in_features\n","        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n","        input_size = 224\n","\n","\n","    elif model_name == \"densenet\":\n","        \"\"\" Densenet121\n","        \"\"\"\n","        model_ft = models.densenet121(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.classifier.in_features\n","        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n","        input_size = 224\n","\n","    elif model_name == \"inception\":\n","        \"\"\" Inception v3\n","        Be careful, expects (299,299) sized images and has auxiliary output\n","        \"\"\"\n","        model_ft = models.inception_v3(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        # Handle the auxilary net\n","        num_ftrs = model_ft.AuxLogits.fc.in_features\n","        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n","        # Handle the primary net\n","        num_ftrs = model_ft.fc.in_features\n","        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n","        input_size = 299\n","\n","    else:\n","        print(\"Invalid model name, exiting...\")\n","        exit()\n","    return model_ft, input_size"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# resnet,vgg,densenet,inception\n","model_name = 'densenet'\n","num_classes = 7\n","feature_extract = False\n","# Initialize the model for this run\n","model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n","# Define the device:\n","device = torch.device('cuda:0')\n","# Put the model on the device:\n","model = model_ft.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# norm_mean = (0.49139968, 0.48215827, 0.44653124)\n","# norm_std = (0.24703233, 0.24348505, 0.26158768)\n","# define the transformation of the train images.\n","train_transform = transforms.Compose([transforms.Resize((input_size,input_size)),transforms.RandomHorizontalFlip(),\n","                                      transforms.RandomVerticalFlip(),transforms.RandomRotation(20),\n","                                      transforms.ColorJitter(brightness=0.1, contrast=0.1, hue=0.1),\n","                                        transforms.ToTensor(), transforms.Normalize(norm_mean, norm_std)])\n","# define the transformation of the val images.\n","val_transform = transforms.Compose([transforms.Resize((input_size,input_size)), transforms.ToTensor(),\n","                                    transforms.Normalize(norm_mean, norm_std)])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define a pytorch dataloader for this dataset\n","class HAM10000(Dataset):\n","    def __init__(self, df, transform=None):\n","        self.df = df\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, index):\n","        # Load data and get label\n","        X = Image.open(self.df['path'][index])\n","        y = torch.tensor(int(self.df['cell_type_idx'][index]))\n","\n","        if self.transform:\n","            X = self.transform(X)\n","\n","        return X, y"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define the training set using the table train_df and using our defined transitions (train_transform)\n","training_set = HAM10000(df_train, transform=train_transform)\n","train_loader = DataLoader(training_set, batch_size=32, shuffle=True, num_workers=4)\n","# Same for the validation set:\n","validation_set = HAM10000(df_val, transform=train_transform)\n","val_loader = DataLoader(validation_set, batch_size=32, shuffle=False, num_workers=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# we use Adam optimizer, use cross entropy loss as our loss function\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","criterion = nn.CrossEntropyLoss().to(device)"]},{"cell_type":"markdown","metadata":{},"source":["## Step 3. Model training"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# this function is used during training process, to calculation the loss and accuracy\n","class AverageMeter(object):\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["total_loss_train, total_acc_train = [],[]\n","def train(train_loader, model, criterion, optimizer, epoch):\n","    model.train()\n","    train_loss = AverageMeter()\n","    train_acc = AverageMeter()\n","    curr_iter = (epoch - 1) * len(train_loader)\n","    for i, data in enumerate(train_loader):\n","        images, labels = data\n","        N = images.size(0)\n","        # print('image shape:',images.size(0), 'label shape',labels.size(0))\n","        images = Variable(images).to(device)\n","        labels = Variable(labels).to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        prediction = outputs.max(1, keepdim=True)[1]\n","        train_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n","        train_loss.update(loss.item())\n","        curr_iter += 1\n","        if (i + 1) % 100 == 0:\n","            print('[epoch %d], [iter %d / %d], [train loss %.5f], [train acc %.5f]' % (\n","                epoch, i + 1, len(train_loader), train_loss.avg, train_acc.avg))\n","            total_loss_train.append(train_loss.avg)\n","            total_acc_train.append(train_acc.avg)\n","    return train_loss.avg, train_acc.avg"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def validate(val_loader, model, criterion, optimizer, epoch):\n","    model.eval()\n","    val_loss = AverageMeter()\n","    val_acc = AverageMeter()\n","    with torch.no_grad():\n","        for i, data in enumerate(val_loader):\n","            images, labels = data\n","            N = images.size(0)\n","            images = Variable(images).to(device)\n","            labels = Variable(labels).to(device)\n","\n","            outputs = model(images)\n","            prediction = outputs.max(1, keepdim=True)[1]\n","\n","            val_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n","\n","            val_loss.update(criterion(outputs, labels).item())\n","\n","    print('------------------------------------------------------------')\n","    print('[epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss.avg, val_acc.avg))\n","    print('------------------------------------------------------------')\n","    return val_loss.avg, val_acc.avg"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["epoch_num = 10\n","best_val_acc = 0\n","total_loss_val, total_acc_val = [],[]\n","for epoch in range(1, epoch_num+1):\n","    loss_train, acc_train = train(train_loader, model, criterion, optimizer, epoch)\n","    loss_val, acc_val = validate(val_loader, model, criterion, optimizer, epoch)\n","    total_loss_val.append(loss_val)\n","    total_acc_val.append(acc_val)\n","    if acc_val > best_val_acc:\n","        best_val_acc = acc_val\n","        print('*****************************************************')\n","        print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, loss_val, acc_val))\n","        print('*****************************************************')"]},{"cell_type":"markdown","metadata":{},"source":["## Step 4. Model evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig = plt.figure(num = 2)\n","fig1 = fig.add_subplot(2,1,1)\n","fig2 = fig.add_subplot(2,1,2)\n","fig1.plot(total_loss_train, label = 'training loss')\n","fig1.plot(total_acc_train, label = 'training accuracy')\n","fig2.plot(total_loss_val, label = 'validation loss')\n","fig2.plot(total_acc_val, label = 'validation accuracy')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.Blues):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, cm[i, j],\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.eval()\n","y_label = []\n","y_predict = []\n","with torch.no_grad():\n","    for i, data in enumerate(val_loader):\n","        images, labels = data\n","        N = images.size(0)\n","        images = Variable(images).to(device)\n","        outputs = model(images)\n","        prediction = outputs.max(1, keepdim=True)[1]\n","        y_label.extend(labels.cpu().numpy())\n","        y_predict.extend(np.squeeze(prediction.cpu().numpy().T))\n","\n","# compute the confusion matrix\n","confusion_mtx = confusion_matrix(y_label, y_predict)\n","# plot the confusion matrix\n","plot_labels = ['akiec', 'bcc', 'bkl', 'df', 'nv', 'vasc','mel']\n","plot_confusion_matrix(confusion_mtx, plot_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Generate a classification report\n","report = classification_report(y_label, y_predict, target_names=plot_labels)\n","print(report)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["label_frac_error = 1 - np.diag(confusion_mtx) / np.sum(confusion_mtx, axis=1)\n","plt.bar(np.arange(7),label_frac_error)\n","plt.xlabel('True Label')\n","plt.ylabel('Fraction classified incorrectly')"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":54339,"sourceId":104884,"sourceType":"datasetVersion"}],"dockerImageVersionId":25160,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
