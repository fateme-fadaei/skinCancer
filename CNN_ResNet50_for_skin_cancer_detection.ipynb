{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 505351,
          "sourceType": "datasetVersion",
          "datasetId": 174469
        }
      ],
      "dockerImageVersionId": 25160,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#importing Essential Libraries"
      ],
      "metadata": {
        "_uuid": "3ce2b3bf606c3bf06d9f61249f580a55812f9e42",
        "id": "3jgfBhkGsyEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from glob import glob\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "np.random.seed(11)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "import itertools\n",
        "\n",
        "import keras\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
        "from keras import backend as K\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras import backend as K\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "gklRIdmXsyEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading pictures and making Dictionary of images and labels\n"
      ],
      "metadata": {
        "_uuid": "c170def1ed6bd1e279dc6d5ae86a95cf6cfd2efb",
        "id": "G6bpGKHRsyEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_benign_train = '../input/data/train/benign'\n",
        "folder_malignant_train = '../input/data/train/malignant'\n",
        "\n",
        "folder_benign_test = '../input/data/test/benign'\n",
        "folder_malignant_test = '../input/data/test/malignant'\n",
        "\n",
        "read = lambda imname: np.asarray(Image.open(imname).convert(\"RGB\"))\n",
        "\n",
        "# Load in training pictures\n",
        "ims_benign = [read(os.path.join(folder_benign_train, filename)) for filename in os.listdir(folder_benign_train)]\n",
        "X_benign = np.array(ims_benign, dtype='uint8')\n",
        "ims_malignant = [read(os.path.join(folder_malignant_train, filename)) for filename in os.listdir(folder_malignant_train)]\n",
        "X_malignant = np.array(ims_malignant, dtype='uint8')\n",
        "\n",
        "# Load in testing pictures\n",
        "ims_benign = [read(os.path.join(folder_benign_test, filename)) for filename in os.listdir(folder_benign_test)]\n",
        "X_benign_test = np.array(ims_benign, dtype='uint8')\n",
        "ims_malignant = [read(os.path.join(folder_malignant_test, filename)) for filename in os.listdir(folder_malignant_test)]\n",
        "X_malignant_test = np.array(ims_malignant, dtype='uint8')\n",
        "\n",
        "# Create labels\n",
        "y_benign = np.zeros(X_benign.shape[0])\n",
        "y_malignant = np.ones(X_malignant.shape[0])\n",
        "\n",
        "y_benign_test = np.zeros(X_benign_test.shape[0])\n",
        "y_malignant_test = np.ones(X_malignant_test.shape[0])\n",
        "\n",
        "\n",
        "# Merge data\n",
        "X_train = np.concatenate((X_benign, X_malignant), axis = 0)\n",
        "y_train = np.concatenate((y_benign, y_malignant), axis = 0)\n",
        "\n",
        "X_test = np.concatenate((X_benign_test, X_malignant_test), axis = 0)\n",
        "y_test = np.concatenate((y_benign_test, y_malignant_test), axis = 0)\n",
        "\n",
        "# Shuffle data\n",
        "s = np.arange(X_train.shape[0])\n",
        "np.random.shuffle(s)\n",
        "X_train = X_train[s]\n",
        "y_train = y_train[s]\n",
        "\n",
        "s = np.arange(X_test.shape[0])\n",
        "np.random.shuffle(s)\n",
        "X_test = X_test[s]\n",
        "y_test = y_test[s]"
      ],
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "id": "X-Hfs0VBsyEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display first 15 images of moles, and how they are classified\n",
        "w=40\n",
        "h=30\n",
        "fig=plt.figure(figsize=(12, 8))\n",
        "columns = 5\n",
        "rows = 3\n",
        "\n",
        "for i in range(1, columns*rows +1):\n",
        "    ax = fig.add_subplot(rows, columns, i)\n",
        "    if y_train[i] == 0:\n",
        "        ax.title.set_text('Benign')\n",
        "    else:\n",
        "        ax.title.set_text('Malignant')\n",
        "    plt.imshow(X_train[i], interpolation='nearest')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "RZUfAkXYsyEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Categorical Labels"
      ],
      "metadata": {
        "id": "J9601BGnsyEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = to_categorical(y_train, num_classes= 2)\n",
        "y_test = to_categorical(y_test, num_classes= 2)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Ur9AKWeAsyEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Normalization"
      ],
      "metadata": {
        "_uuid": "2d1c806e6c6e46916ffb40b5e2848c66c33ed719",
        "id": "vzn12s6xsyEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train/255.\n",
        "X_test = X_test/255."
      ],
      "metadata": {
        "_uuid": "cd19d9fa10edf4cd89f178db0291be76dbdcbfef",
        "trusted": true,
        "id": "cyxpvc-8syEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Building\n",
        "## CNN\n"
      ],
      "metadata": {
        "_uuid": "857f705a561f046a1d63ffa17a8a0b1e8da16ff5",
        "id": "2dn7huYisyEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build(input_shape= (224,224,3), lr = 1e-3, num_classes= 2,\n",
        "          init= 'normal', activ= 'relu', optim= 'adam'):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(64, kernel_size=(3, 3),padding = 'Same',input_shape=input_shape,\n",
        "                     activation= activ, kernel_initializer='glorot_uniform'))\n",
        "    model.add(MaxPool2D(pool_size = (2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Conv2D(64, kernel_size=(3, 3),padding = 'Same',\n",
        "                     activation =activ, kernel_initializer = 'glorot_uniform'))\n",
        "    model.add(MaxPool2D(pool_size = (2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu', kernel_initializer=init))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    model.summary()\n",
        "\n",
        "    if optim == 'rmsprop':\n",
        "        optimizer = RMSprop(lr=lr)\n",
        "\n",
        "    else:\n",
        "        optimizer = Adam(lr=lr)\n",
        "\n",
        "    model.compile(optimizer = optimizer ,loss = \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "# Set a learning rate annealer\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',\n",
        "                                            patience=5,\n",
        "                                            verbose=1,\n",
        "                                            factor=0.5,\n",
        "                                            min_lr=1e-7)\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "Gzw-NYgesyEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (224,224,3)\n",
        "lr = 1e-5\n",
        "init = 'normal'\n",
        "activ = 'relu'\n",
        "optim = 'adam'\n",
        "epochs = 50\n",
        "batch_size = 64\n",
        "\n",
        "model = build(lr=lr, init= init, activ= activ, optim=optim, input_shape= input_shape)\n",
        "\n",
        "history = model.fit(X_train, y_train, validation_split=0.2,\n",
        "                    epochs= epochs, batch_size= batch_size, verbose=0,\n",
        "                    callbacks=[learning_rate_reduction]\n",
        "                   )\n",
        "\n",
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "iP1v7Wl3syEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K.clear_session()\n",
        "del model\n",
        "del history"
      ],
      "metadata": {
        "trusted": true,
        "id": "aIiOvDetsyEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cross-Validating Model\n"
      ],
      "metadata": {
        "id": "L_2V_h9WsyEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define 3-fold cross validation test harness\n",
        "kfold = KFold(n_splits=3, shuffle=True, random_state=11)\n",
        "\n",
        "cvscores = []\n",
        "for train, test in kfold.split(X_train, y_train):\n",
        "  # create model\n",
        "    model = build(lr=lr,\n",
        "                  init= init,\n",
        "                  activ= activ,\n",
        "                  optim=optim,\n",
        "                  input_shape= input_shape)\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(X_train[train], y_train[train], epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "    # evaluate the model\n",
        "    scores = model.evaluate(X_train[test], y_train[test], verbose=0)\n",
        "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "    cvscores.append(scores[1] * 100)\n",
        "    K.clear_session()\n",
        "    del model\n",
        "\n",
        "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
      ],
      "metadata": {
        "_uuid": "55083e7f7d76cb7131b655701021ba2745627c43",
        "trusted": true,
        "id": "YCYEtouPsyEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing the model"
      ],
      "metadata": {
        "id": "gqqAQmalsyER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting model to all data\n",
        "model = build(lr=lr,\n",
        "              init= init,\n",
        "              activ= activ,\n",
        "              optim=optim,\n",
        "              input_shape= input_shape)\n",
        "\n",
        "model.fit(X_train, y_train,\n",
        "          epochs=epochs, batch_size= batch_size, verbose=0,\n",
        "          callbacks=[learning_rate_reduction]\n",
        "         )\n",
        "\n",
        "# Testing model on test data to evaluate\n",
        "y_pred = model.predict_classes(X_test)\n",
        "\n",
        "print(accuracy_score(np.argmax(y_test, axis=1),y_pred))"
      ],
      "metadata": {
        "trusted": true,
        "id": "oB_pqkLHsyER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save model\n",
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")\n",
        "\n",
        "# Clear memory, because of memory overload\n",
        "del model\n",
        "K.clear_session()"
      ],
      "metadata": {
        "trusted": true,
        "id": "Dl9utl77syES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet50\n"
      ],
      "metadata": {
        "id": "OG5UGAetsyES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (224,224,3)\n",
        "lr = 1e-5\n",
        "epochs = 50\n",
        "batch_size = 64\n",
        "\n",
        "model = ResNet50(include_top=True,\n",
        "                 weights= None,\n",
        "                 input_tensor=None,\n",
        "                 input_shape=input_shape,\n",
        "                 pooling='avg',\n",
        "                 classes=2)\n",
        "\n",
        "model.compile(optimizer = Adam(lr) ,\n",
        "              loss = \"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(X_train, y_train, validation_split=0.2,\n",
        "                    epochs= epochs, batch_size= batch_size, verbose=2,\n",
        "                    callbacks=[learning_rate_reduction]\n",
        "                   )\n",
        "\n",
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "nZ32ZR7IsyET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ResNet50 on all the data\n",
        "model.fit(X_train, y_train,\n",
        "          epochs=epochs, batch_size= epochs, verbose=0,\n",
        "          callbacks=[learning_rate_reduction]\n",
        "         )\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "WqcEc2eAsyEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing model on test data to evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))\n",
        "\n",
        "# save model\n",
        "# serialize model to JSON\n",
        "resnet50_json = model.to_json()\n",
        "\n",
        "with open(\"resnet50.json\", \"w\") as json_file:\n",
        "    json_file.write(resnet50_json)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"resnet50.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "z1DAA87ssyEU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}